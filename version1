<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÏïÑÌã∞ÌÅ¥ ÏïÑÏπ¥Ïù¥Î∏å</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            background: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
        }
        
        h1 {
            font-size: 2em;
            margin-bottom: 10px;
        }
        
        .subtitle {
            opacity: 0.9;
            font-size: 0.9em;
        }
        
        .controls {
            padding: 20px 30px;
            background: #f8f9fa;
            border-bottom: 1px solid #e0e0e0;
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
            align-items: center;
        }
        
        .search-box {
            flex: 1;
            min-width: 250px;
        }
        
        .search-box input {
            width: 100%;
            padding: 10px 15px;
            border: 2px solid #ddd;
            border-radius: 5px;
            font-size: 14px;
        }
        
        .search-box input:focus {
            outline: none;
            border-color: #667eea;
        }
        
        .filter-section {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        
        .filter-label {
            font-size: 12px;
            font-weight: 600;
            color: #666;
            text-transform: uppercase;
        }
        
        .filter-buttons {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }
        
        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #ddd;
            background: white;
            border-radius: 5px;
            cursor: pointer;
            font-size: 13px;
            transition: all 0.2s;
        }
        
        .filter-btn:hover {
            border-color: #667eea;
            color: #667eea;
        }
        
        .filter-btn.active {
            background: #667eea;
            border-color: #667eea;
            color: white;
        }
        
        .stats {
            padding: 15px 30px;
            background: #fff3cd;
            border-bottom: 1px solid #e0e0e0;
            font-size: 14px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .sort-options {
            display: flex;
            gap: 10px;
            align-items: center;
        }
        
        .sort-options label {
            font-size: 13px;
            color: #666;
        }
        
        .sort-options select {
            padding: 5px 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 13px;
        }
        
        .articles-grid {
            padding: 30px;
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 20px;
        }
        
        .article-card {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 20px;
            transition: all 0.3s;
            background: white;
        }
        
        .article-card:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            transform: translateY(-2px);
        }
        
        .article-header {
            display: flex;
            justify-content: space-between;
            align-items: start;
            margin-bottom: 12px;
        }
        
        .article-site {
            font-weight: 600;
            color: #667eea;
            font-size: 0.9em;
        }
        
        .article-date {
            color: #666;
            font-size: 0.85em;
        }
        
        .article-title {
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 10px;
            line-height: 1.4;
        }
        
        .article-title a {
            color: #333;
            text-decoration: none;
        }
        
        .article-title a:hover {
            color: #667eea;
        }
        
        .article-summary {
            color: #666;
            font-size: 0.9em;
            line-height: 1.6;
            margin-bottom: 12px;
        }
        
        .article-meta {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }
        
        .tag {
            background: #e9ecef;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.75em;
            color: #495057;
        }
        
        .category-tag {
            background: #d4edda;
            color: #155724;
        }
        
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #999;
        }
        
        .loading-message {
            text-align: center;
            padding: 60px 20px;
            color: #667eea;
            font-size: 1.2em;
        }
        
        .error-message {
            text-align: center;
            padding: 60px 20px;
            color: #dc3545;
            font-size: 1.1em;
        }
        
        @media (max-width: 768px) {
            .articles-grid {
                grid-template-columns: 1fr;
            }
            
            .controls {
                flex-direction: column;
            }
            
            .stats {
                flex-direction: column;
                gap: 10px;
                align-items: flex-start;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üìö ÏïÑÌã∞ÌÅ¥ ÏïÑÏπ¥Ïù¥Î∏å</h1>
            <div class="subtitle">HR, L&D, Leadership ÏµúÏã† Ïù∏ÏÇ¨Ïù¥Ìä∏ Î™®Ïùå</div>
        </header>
        
        <div class="controls">
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="üîç Ï†úÎ™©, ÏöîÏïΩ, ÌÉúÍ∑∏Î°ú Í≤ÄÏÉâ...">
            </div>
            <div class="filter-section">
                <div class="filter-label">ÏÇ¨Ïù¥Ìä∏ ÌïÑÌÑ∞</div>
                <div class="filter-buttons" id="siteFilters">
                    <button class="filter-btn active" data-filter="all">Ï†ÑÏ≤¥</button>
                    <button class="filter-btn" data-filter="Degreed">Degreed</button>
                    <button class="filter-btn" data-filter="Josh Bersin">Josh Bersin</button>
                    <button class="filter-btn" data-filter="SHRM">SHRM</button>
                    <button class="filter-btn" data-filter="Unleash">Unleash</button>
                    <button class="filter-btn" data-filter="DDI">DDI</button>
                    <button class="filter-btn" data-filter="Wharton Knowledge">Wharton</button>
                    <button class="filter-btn" data-filter="Korn Ferry">Korn Ferry</button>
                </div>
            </div>
            <div class="filter-section">
                <div class="filter-label">Ïπ¥ÌÖåÍ≥†Î¶¨ ÌïÑÌÑ∞</div>
                <div class="filter-buttons" id="categoryFilters">
                    <button class="filter-btn active" data-filter-category="all">Ï†ÑÏ≤¥</button>
                    <button class="filter-btn" data-filter-category="L&D Ï†ÑÎûµ Î∞è LX">L&D Ï†ÑÎûµ Î∞è LX</button>
                    <button class="filter-btn" data-filter-category="OD">OD</button>
                    <button class="filter-btn" data-filter-category="TD">TD</button>
                    <button class="filter-btn" data-filter-category="Î¶¨ÎçîÏã≠">Î¶¨ÎçîÏã≠</button>
                    <button class="filter-btn" data-filter-category="Tech">Tech</button>
                    <button class="filter-btn" data-filter-category="Í∏∞ÌÉÄ">Í∏∞ÌÉÄ</button>
                </div>
            </div>
            <div class="filter-section">
                <div class="filter-label">ÎÇ†Ïßú ÌïÑÌÑ∞</div>
                <div class="filter-buttons" id="dateFilters">
                    <button class="filter-btn active" data-filter-date="all">Ï†ÑÏ≤¥</button>
                    <button class="filter-btn" data-filter-date="today">Ïò§Îäò</button>
                    <button class="filter-btn" data-filter-date="week">ÏµúÍ∑º 7Ïùº</button>
                    <button class="filter-btn" data-filter-date="month">ÏµúÍ∑º 30Ïùº</button>
                    <button class="filter-btn" data-filter-date="custom">ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï</button>
                </div>
                <div id="customDateRange" style="display: none; margin-top: 10px; display: flex; gap: 10px; align-items: center;">
                    <input type="date" id="startDate" style="padding: 6px 10px; border: 2px solid #ddd; border-radius: 5px; font-size: 13px;">
                    <span style="color: #666;">~</span>
                    <input type="date" id="endDate" style="padding: 6px 10px; border: 2px solid #ddd; border-radius: 5px; font-size: 13px;">
                </div>
            </div>
        </div>
        
        <div class="stats">
            <div>
                Ï¥ù <strong id="totalCount">0</strong>Í∞úÏùò ÏïÑÌã∞ÌÅ¥
            </div>
            <div class="sort-options">
                <label>Ï†ïÎ†¨:</label>
                <select id="sortSelect">
                    <option value="date-desc">ÏµúÏã†Ïàú</option>
                    <option value="date-asc">Ïò§ÎûòÎêúÏàú</option>
                    <option value="site">ÏÇ¨Ïù¥Ìä∏Î™Ö</option>
                    <option value="title">Ï†úÎ™©</option>
                </select>
            </div>
        </div>
        
        <div class="articles-grid" id="articlesGrid">
            <div class="loading-message">Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§Îäî Ï§ë...</div>
        </div>
    </div>
    
    <script>
        let articlesData = [];
        let currentSiteFilter = 'all';
        let currentCategoryFilter = 'all';
        let currentDateFilter = 'all';
        let customStartDate = null;
        let customEndDate = null;
        let searchTerm = '';
        let currentSort = 'date-desc';
        
        // ÌéòÏù¥ÏßÄ Î°úÎìú Ïãú ÏûêÎèôÏúºÎ°ú articles.json Î∂àÎü¨Ïò§Í∏∞
        window.addEventListener('DOMContentLoaded', async () => {
            try {
                const response = await fetch('articles.json');
                
                if (!response.ok) {
                    throw new Error('articles.json ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.');
                }
                
                articlesData = await response.json();
                
                // Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å ÌõÑ Î†åÎçîÎßÅ
                initializeEventListeners();
                renderArticles();
                
            } catch (error) {
                console.error('Error loading articles:', error);
                document.getElementById('articlesGrid').innerHTML = `
                    <div class="error-message">
                        <strong>Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë Ï§ëÏûÖÎãàÎã§...</strong><br><br>
                        ÏïÑÏßÅ ÏïÑÌã∞ÌÅ¥ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï§ÄÎπÑÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.<br>
                        scraper.pyÎ•º Ïã§ÌñâÌïòÏó¨ articles.json ÌååÏùºÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    </div>
                `;
            }
        });
        
        function isWithinDateRange(articleDate, filter) {
            const articleDateObj = new Date(articleDate);
            const today = new Date();
            today.setHours(0, 0, 0, 0);
            
            switch(filter) {
                case 'all':
                    return true;
                case 'today':
                    const todayStr = today.toISOString().split('T')[0];
                    return articleDate === todayStr;
                case 'week':
                    const weekAgo = new Date(today);
                    weekAgo.setDate(weekAgo.getDate() - 7);
                    return articleDateObj >= weekAgo;
                case 'month':
                    const monthAgo = new Date(today);
                    monthAgo.setDate(monthAgo.getDate() - 30);
                    return articleDateObj >= monthAgo;
                case 'custom':
                    if (!customStartDate && !customEndDate) return true;
                    const start = customStartDate ? new Date(customStartDate) : new Date('1900-01-01');
                    const end = customEndDate ? new Date(customEndDate) : new Date('2100-12-31');
                    return articleDateObj >= start && articleDateObj <= end;
                default:
                    return true;
            }
        }
        
        function sortArticles(articles, sortBy) {
            const sorted = [...articles];
            switch(sortBy) {
                case 'date-desc':
                    return sorted.sort((a, b) => new Date(b.date) - new Date(a.date));
                case 'date-asc':
                    return sorted.sort((a, b) => new Date(a.date) - new Date(b.date));
                case 'site':
                    return sorted.sort((a, b) => a.site.localeCompare(b.site));
                case 'title':
                    return sorted.sort((a, b) => a.title.localeCompare(b.title));
                default:
                    return sorted;
            }
        }
        
        function renderArticles() {
            const grid = document.getElementById('articlesGrid');
            let filtered = articlesData.filter(article => {
                const matchesSite = currentSiteFilter === 'all' || article.site === currentSiteFilter;
                const matchesCategory = currentCategoryFilter === 'all' || article.category === currentCategoryFilter;
                const matchesDate = isWithinDateRange(article.date, currentDateFilter);
                const matchesSearch = searchTerm === '' || 
                    article.title.toLowerCase().includes(searchTerm.toLowerCase()) ||
                    article.summary.toLowerCase().includes(searchTerm.toLowerCase()) ||
                    article.tags.some(tag => tag.toLowerCase().includes(searchTerm.toLowerCase()));
                return matchesSite && matchesCategory && matchesDate && matchesSearch;
            });
            
            filtered = sortArticles(filtered, currentSort);
            
            document.getElementById('totalCount').textContent = filtered.length;
            
            if (filtered.length === 0) {
                grid.innerHTML = '<div class="no-results">Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.</div>';
                return;
            }
            
            grid.innerHTML = filtered.map(article => `
                <div class="article-card">
                    <div class="article-header">
                        <span class="article-site">${article.site}</span>
                        <span class="article-date">${article.date}</span>
                    </div>
                    <div class="article-title">
                        <a href="${article.url}" target="_blank">${article.title}</a>
                    </div>
                    <div class="article-summary">${article.summary}</div>
                    <div class="article-meta">
                        <span class="tag category-tag">${article.category}</span>
                        ${article.tags.map(tag => `<span class="tag">${tag}</span>`).join('')}
                    </div>
                </div>
            `).join('');
        }
        
        function initializeEventListeners() {
            // Search functionality
            document.getElementById('searchInput').addEventListener('input', (e) => {
                searchTerm = e.target.value;
                renderArticles();
            });
            
            // Site filter functionality
            document.querySelectorAll('#siteFilters .filter-btn').forEach(btn => {
                btn.addEventListener('click', (e) => {
                    document.querySelectorAll('#siteFilters .filter-btn').forEach(b => b.classList.remove('active'));
                    e.target.classList.add('active');
                    currentSiteFilter = e.target.dataset.filter;
                    renderArticles();
                });
            });
            
            // Category filter functionality
            document.querySelectorAll('#categoryFilters .filter-btn').forEach(btn => {
                btn.addEventListener('click', (e) => {
                    document.querySelectorAll('#categoryFilters .filter-btn').forEach(b => b.classList.remove('active'));
                    e.target.classList.add('active');
                    currentCategoryFilter = e.target.dataset.filterCategory;
                    renderArticles();
                });
            });
            
            // Date filter functionality
            document.querySelectorAll('#dateFilters .filter-btn').forEach(btn => {
                btn.addEventListener('click', (e) => {
                    document.querySelectorAll('#dateFilters .filter-btn').forEach(b => b.classList.remove('active'));
                    e.target.classList.add('active');
                    currentDateFilter = e.target.dataset.filterDate;
                    
                    // Show/hide custom date range inputs
                    const customDateRange = document.getElementById('customDateRange');
                    if (currentDateFilter === 'custom') {
                        customDateRange.style.display = 'flex';
                    } else {
                        customDateRange.style.display = 'none';
                    }
                    
                    renderArticles();
                });
            });
            
            // Custom date range listeners
            document.getElementById('startDate').addEventListener('change', (e) => {
                customStartDate = e.target.value;
                if (currentDateFilter === 'custom') {
                    renderArticles();
                }
            });
            
            document.getElementById('endDate').addEventListener('change', (e) => {
                customEndDate = e.target.value;
                if (currentDateFilter === 'custom') {
                    renderArticles();
                }
            });
            
            // Sort functionality
            document.getElementById('sortSelect').addEventListener('change', (e) => {
                currentSort = e.target.value;
                renderArticles();
            });
        }
    </script>
</body>
</html>

# üìö L&D ÏïÑÌã∞ÌÅ¥ ÏïÑÏπ¥Ïù¥Î∏å

L&D, HR, Leadership Í¥ÄÎ†® ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏµúÏã† ÏïÑÌã∞ÌÅ¥ÏùÑ ÏûêÎèôÏúºÎ°ú ÏàòÏßëÌïòÍ≥† ÏïÑÏπ¥Ïù¥ÎπôÌïòÎäî ÏãúÏä§ÌÖúÏûÖÎãàÎã§.

## üåü Ï£ºÏöî Í∏∞Îä•

- ‚úÖ **ÏûêÎèô Ïä§ÌÅ¨ÎûòÌïë**: GitHub ActionsÎ°ú Îß§Ïùº Ïò§Ï†Ñ 9Ïãú ÏûêÎèô Ïã§Ìñâ
- ‚úÖ **7Í∞ú ÏÇ¨Ïù¥Ìä∏ Î™®ÎãàÌÑ∞ÎßÅ**: Degreed, Josh Bersin, SHRM, Unleash, DDI, Wharton, Korn Ferry
- ‚úÖ **Ïã§ÏãúÍ∞Ñ ÌïÑÌÑ∞ÎßÅ**: ÏÇ¨Ïù¥Ìä∏Î≥Ñ, Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ, ÎÇ†ÏßúÎ≥Ñ ÌïÑÌÑ∞
- ‚úÖ **Í≤ÄÏÉâ Í∏∞Îä•**: Ï†úÎ™©, ÏöîÏïΩ, ÌÉúÍ∑∏ Í≤ÄÏÉâ
- ‚úÖ **Î∞òÏùëÌòï ÎîîÏûêÏù∏**: Î™®Î∞îÏùº, ÌÉúÎ∏îÎ¶ø, Îç∞Ïä§ÌÅ¨ÌÜ± ÏßÄÏõê

## üìÅ ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞

```
ld-article-archive/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ scrape.yml          # GitHub Actions ÏûêÎèôÌôî ÏÑ§Ï†ï
‚îú‚îÄ‚îÄ index.html                  # Î©îÏù∏ ÏõπÌéòÏù¥ÏßÄ
‚îú‚îÄ‚îÄ scraper.py                  # ÏïÑÌã∞ÌÅ¥ ÏàòÏßë Ïä§ÌÅ¨Î¶ΩÌä∏
‚îú‚îÄ‚îÄ articles.json               # ÏàòÏßëÎêú ÏïÑÌã∞ÌÅ¥ Îç∞Ïù¥ÌÑ∞
‚îú‚îÄ‚îÄ requirements.txt            # Python Ìå®ÌÇ§ÏßÄ Î™©Î°ù
‚îî‚îÄ‚îÄ README.md                   # ÌîÑÎ°úÏ†ùÌä∏ ÏÑ§Î™ÖÏÑú
```

## üöÄ ÏãúÏûëÌïòÍ∏∞

### 1. Î°úÏª¨ÏóêÏÑú Ïã§Ìñâ

```bash
# Ï†ÄÏû•ÏÜå ÌÅ¥Î°†
git clone https://github.com/yourusername/ld-article-archive.git
cd ld-article-archive

# Python Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò
pip install -r requirements.txt

# Ïä§ÌÅ¨ÎûòÌçº Ïã§Ìñâ
python scraper.py

# Î°úÏª¨ ÏÑúÎ≤Ñ Ïã§Ìñâ
python -m http.server 8000

# Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú Ïó¥Í∏∞
# http://localhost:8000
```

### 2. GitHub PagesÎ°ú Î∞∞Ìè¨

1. GitHubÏóê Ï†ÄÏû•ÏÜå ÏóÖÎ°úÎìú
2. Settings > PagesÎ°ú Ïù¥Îèô
3. Source: Deploy from a branch
4. Branch: main, Folder: / (root)
5. Save ÌÅ¥Î¶≠

Î∞∞Ìè¨ ÌõÑ Ï†ëÏÜç: `https://yourusername.github.io/ld-article-archive/`

### 3. NetlifyÎ°ú Î∞∞Ìè¨

1. [Netlify](https://www.netlify.com/)Ïóê Î°úÍ∑∏Ïù∏
2. "New site from Git" ÌÅ¥Î¶≠
3. GitHub Ï†ÄÏû•ÏÜå Ïó∞Í≤∞
4. Build settings:
   - Build command: (ÎπÑÏõåÎëêÍ∏∞)
   - Publish directory: `/`
5. Deploy ÌÅ¥Î¶≠

## ‚öôÔ∏è GitHub Actions ÏûêÎèôÌôî

### ÏÑ§Ï†ï Î∞©Î≤ï

Ïù¥ÎØ∏ `.github/workflows/scrape.yml` ÌååÏùºÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏñ¥ ÏûêÎèôÏúºÎ°ú ÏûëÎèôÌï©ÎãàÎã§.

**ÏûêÎèô Ïã§Ìñâ ÏãúÍ∞Ñ**: Îß§Ïùº Ïò§Ï†Ñ 9Ïãú (ÌïúÍµ≠ ÏãúÍ∞Ñ, UTC 0Ïãú)

### ÏàòÎèô Ïã§Ìñâ

1. GitHub Ï†ÄÏû•ÏÜå ÌéòÏù¥ÏßÄ Ïù¥Îèô
2. "Actions" ÌÉ≠ ÌÅ¥Î¶≠
3. "Daily Article Scraper" ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÑ†ÌÉù
4. "Run workflow" Î≤ÑÌäº ÌÅ¥Î¶≠

### ÏûëÎèô Î∞©Ïãù

1. **Îß§Ïùº Ïò§Ï†Ñ 9Ïãú**: GitHub ActionsÍ∞Ä ÏûêÎèôÏúºÎ°ú Ïã§Ìñâ
2. **Ïä§ÌÅ¨ÎûòÌçº Ïã§Ìñâ**: `scraper.py`Í∞Ä ÏµúÏã† ÏïÑÌã∞ÌÅ¥ ÏàòÏßë
3. **Î≥ÄÍ≤ΩÏÇ¨Ìï≠ ÌôïÏù∏**: `articles.json` ÌååÏùºÏù¥ ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
4. **ÏûêÎèô Ïª§Î∞ã**: Î≥ÄÍ≤ΩÏÇ¨Ìï≠Ïù¥ ÏûàÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú Ïª§Î∞ã & Ìë∏Ïãú
5. **ÏõπÏÇ¨Ïù¥Ìä∏ ÏóÖÎç∞Ïù¥Ìä∏**: GitHub Pages/NetlifyÍ∞Ä ÏûêÎèôÏúºÎ°ú Ïû¨Î∞∞Ìè¨

## üìä Î™®ÎãàÌÑ∞ÎßÅ ÎåÄÏÉÅ ÏÇ¨Ïù¥Ìä∏

| ÏÇ¨Ïù¥Ìä∏ | URL | Ïπ¥ÌÖåÍ≥†Î¶¨ |
|--------|-----|----------|
| Degreed | https://degreed.com/experience/blog/ | L&D Ï†ÑÎûµ Î∞è LX |
| Josh Bersin | https://joshbersin.com/ | TD |
| SHRM | https://www.shrm.org/topics-tools/news | Í∏∞ÌÉÄ |
| Unleash | https://www.unleash.ai/learning-and-development/ | L&D Ï†ÑÎûµ Î∞è LX |
| DDI | https://www.ddi.com/blogs | Î¶¨ÎçîÏã≠ |
| Wharton Knowledge | https://knowledge.wharton.upenn.edu/category/leadership/ | OD |
| Korn Ferry | https://www.kornferry.com/insights | TD |

## üé® Ïπ¥ÌÖåÍ≥†Î¶¨

- **L&D Ï†ÑÎûµ Î∞è LX**: ÌïôÏäµ Ï†ÑÎûµ, ÌïôÏäµ Í≤ΩÌóò ÎîîÏûêÏù∏
- **OD**: Ï°∞ÏßÅ Í∞úÎ∞ú, Î≥ÄÌôî Í¥ÄÎ¶¨
- **TD**: Ïù∏Ïû¨ Í∞úÎ∞ú, Ïä§ÌÇ¨ Í∏∞Î∞ò Ï†ëÍ∑º
- **Î¶¨ÎçîÏã≠**: Î¶¨ÎçîÏã≠ Í∞úÎ∞ú, ÏΩîÏπ≠
- **Tech**: LXP, AI, ÌïôÏäµ Î∂ÑÏÑù
- **Í∏∞ÌÉÄ**: Í∏∞ÌÉÄ HR Í¥ÄÎ†® ÏΩòÌÖêÏ∏†

## üõ†Ô∏è Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ïßï

### ÏÉàÎ°úÏö¥ ÏÇ¨Ïù¥Ìä∏ Ï∂îÍ∞Ä

`scraper.py` ÌååÏùºÏùÑ ÏàòÏ†ïÌïòÏó¨ ÏÉàÎ°úÏö¥ ÏÇ¨Ïù¥Ìä∏Î•º Ï∂îÍ∞ÄÌï† Ïàò ÏûàÏäµÎãàÎã§:

```python
def scrape_new_site(self):
    """Scrape New Site"""
    try:
        url = "https://newsite.com/articles"
        response = requests.get(url, headers=self.headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ÏïÑÌã∞ÌÅ¥ ÏöîÏÜå Ï∞æÍ∏∞
        articles = soup.find_all('article', limit=10)
        
        for article in articles:
            # Ï†úÎ™©, ÎßÅÌÅ¨, ÎÇ†Ïßú, ÏöîÏïΩ Ï∂îÏ∂ú
            # ...
            
            self.articles.append({
                'date': date,
                'site': 'New Site',
                'title': title,
                'url': link,
                'summary': summary,
                'category': 'Your Category',
                'tags': ['tag1', 'tag2']
            })
    except Exception as e:
        print(f"Error scraping New Site: {e}")
```

### Ïä§ÌÅ¨ÎûòÌïë ÏãúÍ∞Ñ Î≥ÄÍ≤Ω

`.github/workflows/scrape.yml` ÌååÏùºÏóêÏÑú cron ÌëúÌòÑÏãù ÏàòÏ†ï:

```yaml
schedule:
  # Îß§Ïùº Ïò§ÌõÑ 2Ïãú (UTC 5Ïãú)
  - cron: '0 5 * * *'
```

[Cron ÌëúÌòÑÏãù ÏÉùÏÑ±Í∏∞](https://crontab.guru/)

## üîß Î¨∏Ï†ú Ìï¥Í≤∞

### Ïä§ÌÅ¨ÎûòÌïëÏù¥ Ïã§Ìå®ÌïòÎäî Í≤ΩÏö∞

1. **ÎÑ§Ìä∏ÏõåÌÅ¨ Î¨∏Ï†ú**: GitHub Actions Î°úÍ∑∏ ÌôïÏù∏
2. **HTML Íµ¨Ï°∞ Î≥ÄÍ≤Ω**: ÏõπÏÇ¨Ïù¥Ìä∏Ïùò HTML ÏÑ†ÌÉùÏûê ÏóÖÎç∞Ïù¥Ìä∏ ÌïÑÏöî
3. **Ï†ëÍ∑º Ï∞®Îã®**: User-Agent Î≥ÄÍ≤Ω ÎòêÎäî ÏöîÏ≤≠ Í∞ÑÍ≤© Ï°∞Ï†ï

### GitHub ActionsÍ∞Ä ÏûëÎèôÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞

1. **Í∂åÌïú ÌôïÏù∏**: Settings > Actions > General
   - "Read and write permissions" ÏÑ†ÌÉù
   - "Allow GitHub Actions to create and approve pull requests" Ï≤¥ÌÅ¨
2. **ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌôúÏÑ±Ìôî**: Actions ÌÉ≠ÏóêÏÑú ÏõåÌÅ¨ÌîåÎ°úÏö∞ Enable ÌôïÏù∏

### Î°úÏª¨ÏóêÏÑú CORS ÏóêÎü¨Í∞Ä Î∞úÏÉùÌïòÎäî Í≤ΩÏö∞

ÌååÏùºÏùÑ ÏßÅÏ†ë Ïó¥ÏßÄ ÎßêÍ≥† Î°úÏª¨ ÏÑúÎ≤ÑÎ•º Ïã§ÌñâÌïòÏÑ∏Ïöî:

```bash
# Python 3
python -m http.server 8000

# Python 2
python -m SimpleHTTPServer 8000

# Node.js (npx ÏÇ¨Ïö©)
npx http-server
```

## üìÑ ÎùºÏù¥ÏÑ†Ïä§

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî Í∞úÏù∏ Î∞è ÏÉÅÏóÖÏ†Å Ïö©ÎèÑÎ°ú ÏûêÏú†Î°≠Í≤å ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.

## ü§ù Í∏∞Ïó¨

Í∞úÏÑ† ÏÇ¨Ìï≠Ïù¥ÎÇò Î≤ÑÍ∑∏ Î¶¨Ìè¨Ìä∏Îäî Ïñ∏Ï†úÎì† ÌôòÏòÅÌï©ÎãàÎã§!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìû Î¨∏Ïùò

ÌîÑÎ°úÏ†ùÌä∏Ïóê ÎåÄÌïú ÏßàÎ¨∏Ïù¥ÎÇò Ï†úÏïàÏÇ¨Ìï≠Ïù¥ ÏûàÏúºÏãúÎ©¥ IssueÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

---

Made with ‚ù§Ô∏è for L&D Professionals

requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0

"""
L&D Article Scraper
Collects articles from 7 major L&D and HR websites
Saves data to articles.json for web display
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import re
from urllib.parse import urljoin

class LDArticleScraper:
    def __init__(self):
        self.articles = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def scrape_degreed(self):
        """Scrape Degreed blog"""
        try:
            url = "https://degreed.com/experience/blog/"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('article', limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    link_elem = article.find('a', href=True)
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        link = urljoin(url, link_elem['href'])
                        
                        date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                        date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                        
                        summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                        summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                        
                        self.articles.append({
                            'date': date,
                            'site': 'Degreed',
                            'title': title,
                            'url': link,
                            'summary': summary,
                            'category': 'L&D Ï†ÑÎûµ Î∞è LX',
                            'tags': ['L&D', 'Degreed']
                        })
                except Exception as e:
                    print(f"Error parsing Degreed article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping Degreed: {e}")
    
    def scrape_josh_bersin(self):
        """Scrape Josh Bersin blog"""
        try:
            url = "https://joshbersin.com/"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('post|article|entry', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1', 'a'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True) or title_elem.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem and 'href' in link_elem.attrs else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|content', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'Josh Bersin',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'TD',
                        'tags': ['TD', 'Josh Bersin', 'Talent Management']
                    })
                except Exception as e:
                    print(f"Error parsing Josh Bersin article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping Josh Bersin: {e}")
    
    def scrape_shrm(self):
        """Scrape SHRM news"""
        try:
            url = "https://www.shrm.org/topics-tools/news"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('article|news|post', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'SHRM',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'Í∏∞ÌÉÄ',
                        'tags': ['HR', 'SHRM', 'News']
                    })
                except Exception as e:
                    print(f"Error parsing SHRM article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping SHRM: {e}")
    
    def scrape_unleash(self):
        """Scrape Unleash L&D"""
        try:
            url = "https://www.unleash.ai/learning-and-development/"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('post|article|card', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'Unleash',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'L&D Ï†ÑÎûµ Î∞è LX',
                        'tags': ['L&D', 'Unleash', 'HR Tech']
                    })
                except Exception as e:
                    print(f"Error parsing Unleash article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping Unleash: {e}")
    
    def scrape_ddi(self):
        """Scrape DDI blogs"""
        try:
            url = "https://www.ddi.com/blogs"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('blog|post|article', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'DDI',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'Î¶¨ÎçîÏã≠',
                        'tags': ['Leadership', 'DDI', 'Development']
                    })
                except Exception as e:
                    print(f"Error parsing DDI article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping DDI: {e}")
    
    def scrape_wharton(self):
        """Scrape Wharton Knowledge"""
        try:
            url = "https://knowledge.wharton.upenn.edu/category/leadership/"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('post|article', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'Wharton Knowledge',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'OD',
                        'tags': ['OD', 'Wharton', 'Research']
                    })
                except Exception as e:
                    print(f"Error parsing Wharton article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping Wharton: {e}")
    
    def scrape_kornferry(self):
        """Scrape Korn Ferry insights"""
        try:
            url = "https://www.kornferry.com/insights"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all(['article', 'div'], class_=re.compile('insight|article|card', re.I), limit=10)
            
            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3', 'h1'])
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    link_elem = article.find('a', href=True)
                    link = urljoin(url, link_elem['href']) if link_elem else url
                    
                    date_elem = article.find(['time', 'span'], class_=re.compile('date|time', re.I))
                    date = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    summary_elem = article.find(['p', 'div'], class_=re.compile('excerpt|summary|description', re.I))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title
                    
                    self.articles.append({
                        'date': date,
                        'site': 'Korn Ferry',
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'category': 'TD',
                        'tags': ['TD', 'Korn Ferry', 'Talent']
                    })
                except Exception as e:
                    print(f"Error parsing Korn Ferry article: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping Korn Ferry: {e}")
    
    def scrape_all(self):
        """Scrape all websites"""
        print("Starting to scrape all websites...")
        
        self.scrape_degreed()
        time.sleep(2)
        
        self.scrape_josh_bersin()
        time.sleep(2)
        
        self.scrape_shrm()
        time.sleep(2)
        
        self.scrape_unleash()
        time.sleep(2)
        
        self.scrape_ddi()
        time.sleep(2)
        
        self.scrape_wharton()
        time.sleep(2)
        
        self.scrape_kornferry()
        
        print(f"Scraping completed. Total articles: {len(self.articles)}")
        return self.articles
    
    def save_to_json(self, filename='articles.json'):
        """Save articles to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.articles, f, ensure_ascii=False, indent=2)
        print(f"Articles saved to {filename}")


if __name__ == "__main__":
    scraper = LDArticleScraper()
    scraper.scrape_all()
    scraper.save_to_json('articles.json')
    print("‚úÖ Scraping complete! Check articles.json")
[
    {
        "title": "Portugu√™s (Portuguese (Brazil))",
        "link": "https://degreed.com/experience/pt-br/blog/",
        "source": "Degreed"
    },
    {
        "title": "Why DegreedPartner with an end-user-obsessed innovator who knows exactly how to deliver effective skills-first learning.",
        "link": "https://degreed.com/experience/why/",
        "source": "Degreed"
    },
    {
        "title": "Our PlatformUnlock lasting skill-building solutions for every employee, from onboarding to retention.",
        "link": "https://degreed.com/experience/our-platform/",
        "source": "Degreed"
    },
    {
        "title": "PartnershipsDegreed partners with the world‚Äôs most forward-thinking and trusted vendors to create true skills-first learning.",
        "link": "https://degreed.com/experience/partnership/",
        "source": "Degreed"
    },
    {
        "title": "Success Stories Hear from thriving companies and individuals who embraced a new kind of learning and saw the results.",
        "link": "https://degreed.com/experience/success-stories/",
        "source": "Degreed"
    },
    {
        "title": "Josh Bersin‚Äôs 2026 Imperatives: Unlock the roadmap for enterprise AI, exclusively through Galileo",
        "link": "https://getgalileo.ai/pricing",
        "source": "JoshBersin"
    },
    {
        "title": "Join us at Irresistible 2026 to unlock exclusive research and strategy sessions.",
        "link": "https://www.uscmarshallexeced.org/event/Irresistible2026/home",
        "source": "JoshBersin"
    },
    {
        "title": "Maximizing the Impact of AI on HR Series",
        "link": "https://joshbersin.com/maximizing-the-impact-of-ai-on-hr/",
        "source": "JoshBersin"
    },
    {
        "title": "Maximizing the Impact of AI in the Age of the Superworker",
        "link": "https://joshbersin.com/maximizing-the-impact-of-ai-in-the-age-of-the-superworker/",
        "source": "JoshBersin"
    },
    {
        "title": "Maximizing the Impact of AI on Core HR, Time Management, and Payroll",
        "link": "https://joshbersin.com/maximizing-the-impact-of-ai-on-core-hr/",
        "source": "JoshBersin"
    },
    {
        "title": "SHRM26 Annual Conference & Expo",
        "link": "https://annual.shrm.org",
        "source": "SHRM"
    },
    {
        "title": "SHRM26 Annual Conference & Expo",
        "link": "https://annual.shrm.org",
        "source": "SHRM"
    },
    {
        "title": "UNLEASH America Highlights",
        "link": "https://www.unleash.ai/unleashamerica/content-hub/",
        "source": "Unleash"
    },
    {
        "title": "UNLEASH America 202617-19 March 2026",
        "link": "https://www.unleash.ai/unleashamerica/",
        "source": "Unleash"
    },
    {
        "title": "UNLEASH World 202620-22 October 2026",
        "link": "https://www.unleash.ai/unleashworld",
        "source": "Unleash"
    },
    {
        "title": "Embrace the future of work",
        "link": "https://www.unleash.ai/events/hr-technology/embrace-the-future-of-work-with-a-transformational-employee-experience/",
        "source": "Unleash"
    },
    {
        "title": "Harnessing the voice of the employee",
        "link": "https://www.unleash.ai/events/employee-experience-and-engagement/power-to-the-people-harnessing-the-voice-of-the-employee/",
        "source": "Unleash"
    },
    {
        "title": "Group Machine Readable Files",
        "link": "https://mrfdata.hmhs.com/",
        "source": "DDI"
    },
    {
        "title": "Nano Tools for Leaders Series",
        "link": "https://knowledge.wharton.upenn.edu/nano-tools-for-leaders-series/",
        "source": "Wharton"
    },
    {
        "title": "The Future of Financial TechnologyDecember 2, 2024",
        "link": "https://knowledge.wharton.upenn.edu/article/the-future-of-financial-technology/",
        "source": "Wharton"
    },
    {
        "title": "The Regulatory Challenges of AI in FinanceNovember 25, 2024",
        "link": "https://knowledge.wharton.upenn.edu/article/the-regulatory-challenges-of-ai-in-finance/",
        "source": "Wharton"
    },
    {
        "title": "New Phenomena in Behavioral and Social InvestingNovember 18, 2024",
        "link": "https://knowledge.wharton.upenn.edu/article/new-phenomena-in-behavioral-and-social-investing/",
        "source": "Wharton"
    },
    {
        "title": "The Future of BankingNovember 11, 2024",
        "link": "https://knowledge.wharton.upenn.edu/article/the-future-of-banking/",
        "source": "Wharton"
    }
]
